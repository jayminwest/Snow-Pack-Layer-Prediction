{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 10:13:43.346426: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-10 10:13:43.400255: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-10 10:13:43.401548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 10:13:44.344646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import importlib, os, utils, urllib\n",
    "from datetime import datetime, timedelta\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(utils)\n",
    "# all_zones_df = utils.clean_raw_webscraper_data('output_data/All_Zones_2021-22_Season_reports_data.csv')\n",
    "# all_zones_df = utils.add_weather_to_reports(all_zones_df)\n",
    "# all_zones_df.to_csv('output_data/All_Zones_2021-22_Season_weather_and_reports_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_zones_df = pd.read_csv('output_data/all_zones_all_data.csv')\n",
    "all_zones_df = pd.read_csv('input_data/Stevens_Pass_Current_Season_reports_data.csv')\n",
    "\n",
    "data = all_zones_df.drop(columns=['bottom_line_text', 'problem_type_text', 'forecast_discussion_text'])\n",
    "\n",
    "data['combined_text'] = data['combined_text'].astype(str)\n",
    "data['combined_text'] = data['combined_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "data.rename(columns={'combined_text': 'tokens'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>zone</th>\n",
       "      <th>overall_risk</th>\n",
       "      <th>above_treeline_risk</th>\n",
       "      <th>near_treeline_risk</th>\n",
       "      <th>below_treeline_risk</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>7.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>165.1</td>\n",
       "      <td>1018.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[warming, temperature, sun, maintain, slight, ...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.3</td>\n",
       "      <td>1016.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[warming, temperature, sun, maintain, threat, ...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>244.6</td>\n",
       "      <td>1015.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mixed, cloud, sun, maintain, chance, triggeri...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>251.1</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[clearing, sky, bring, chance, triggering, wet...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>223.2</td>\n",
       "      <td>1015.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[trigger, avalanche, recent, snow, fell, weeke...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>240.2</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[forecast, updated, reflect, higher, expected,...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>240.2</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[rapid, doubling, snowpack, accompanied, wind,...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>228.4</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[avalanche, danger, increase, storm, deposit, ...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>189.4</td>\n",
       "      <td>1026.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>stevens pass</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[inch, new, snow, wind, created, chance, trigg...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>167.6</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date          zone  overall_risk  above_treeline_risk   \n",
       "0    2023-04-15  stevens pass           1.0                  1.0  \\\n",
       "1    2023-04-14  stevens pass           1.0                  1.0   \n",
       "2    2023-04-13  stevens pass           2.0                  2.0   \n",
       "3    2023-04-12  stevens pass           2.0                  2.0   \n",
       "4    2023-04-11  stevens pass           2.0                  2.0   \n",
       "..          ...           ...           ...                  ...   \n",
       "140  2022-11-28  stevens pass           2.0                  2.0   \n",
       "141  2022-11-28  stevens pass           3.0                  3.0   \n",
       "142  2022-11-27  stevens pass           2.0                  2.0   \n",
       "143  2022-11-26  stevens pass           3.0                  3.0   \n",
       "144  2022-11-25  stevens pass           2.0                  2.0   \n",
       "\n",
       "     near_treeline_risk  below_treeline_risk   \n",
       "0                   1.0                  1.0  \\\n",
       "1                   1.0                  1.0   \n",
       "2                   2.0                  1.0   \n",
       "3                   2.0                  1.0   \n",
       "4                   2.0                  2.0   \n",
       "..                  ...                  ...   \n",
       "140                 2.0                  1.0   \n",
       "141                 2.0                  1.0   \n",
       "142                 2.0                  1.0   \n",
       "143                 3.0                  2.0   \n",
       "144                 2.0                  1.0   \n",
       "\n",
       "                                                tokens  tavg  tmin  tmax   \n",
       "0                                                [nan]   7.6   3.1  11.1  \\\n",
       "1    [warming, temperature, sun, maintain, slight, ...   6.8   0.3  13.0   \n",
       "2    [warming, temperature, sun, maintain, threat, ...   5.9   1.4  10.5   \n",
       "3    [mixed, cloud, sun, maintain, chance, triggeri...   5.4   0.5  10.7   \n",
       "4    [clearing, sky, bring, chance, triggering, wet...   5.8   2.1   9.7   \n",
       "..                                                 ...   ...   ...   ...   \n",
       "140  [trigger, avalanche, recent, snow, fell, weeke...   0.1  -3.7   2.5   \n",
       "141  [forecast, updated, reflect, higher, expected,...   0.1  -3.7   2.5   \n",
       "142  [rapid, doubling, snowpack, accompanied, wind,...   3.4   0.2   6.4   \n",
       "143  [avalanche, danger, increase, storm, deposit, ...   2.8  -0.9   5.9   \n",
       "144  [inch, new, snow, wind, created, chance, trigg...   4.0   0.3   7.1   \n",
       "\n",
       "     prcp   wdir    pres  tsun  \n",
       "0     1.3  165.1  1018.9   0.0  \n",
       "1     0.0  254.3  1016.4   0.0  \n",
       "2     1.1  244.6  1015.9   0.0  \n",
       "3     1.1  251.1  1017.0   0.0  \n",
       "4     5.8  223.2  1015.3   0.0  \n",
       "..    ...    ...     ...   ...  \n",
       "140   2.9  240.2  1011.9   0.0  \n",
       "141   2.9  240.2  1011.9   0.0  \n",
       "142   7.1  228.4  1013.0   0.0  \n",
       "143   2.0  189.4  1026.6   0.0  \n",
       "144  11.3  167.6  1023.9   0.0  \n",
       "\n",
       "[145 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('area', 0.9997828602790833),\n",
       " ('like', 0.9997230172157288),\n",
       " ('day', 0.9997174739837646),\n",
       " ('travel', 0.9997053742408752),\n",
       " ('especially', 0.9997028112411499),\n",
       " ('one', 0.9996917247772217),\n",
       " ('snow', 0.999691367149353),\n",
       " ('tree', 0.9996909499168396),\n",
       " ('find', 0.9996904730796814),\n",
       " ('snowpack', 0.9996773600578308)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the word to vec model:\n",
    "model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save('models/first_word2vec.model')\n",
    "\n",
    "model = Word2Vec.load('models/first_word2vec.model')\n",
    "# Create word embeddings lookup dictionary\n",
    "word_embeddings = {}\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    word_embeddings[word] = model.wv[word]\n",
    "\n",
    "word = 'cornice'\n",
    "model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 11:29:36.329952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-10 11:29:36.334465: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-10 11:29:36.337415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-10 11:29:36.867217: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-10 11:29:36.870039: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-10 11:29:36.871863: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-10 11:29:37.839983: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-10 11:29:37.842979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-10 11:29:37.845490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 96ms/step - loss: 5.4048\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 4.0670\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.8193\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5038\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.4901\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.4909\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.4899\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4979\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.4952\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.4929\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.4932\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.5065\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.4975\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.4926\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.4901\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.4924\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4996\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.4950\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.4965\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a8c42e110>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "# Load the dataframe\n",
    "# df = pd.read_csv('your_dataframe.csv')\n",
    "df = data\n",
    "\n",
    "# Select the features you want to use for training\n",
    "features = ['tavg', 'tmin', 'tmax', 'prcp', 'wdir', 'pres', 'tsun']\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])  # Normalize the numerical features\n",
    "\n",
    "# Convert text tokens to numerical sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tokens'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['tokens'])\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "\n",
    "tokenized_text = [str(tokens).split() for tokens in df['tokens']]\n",
    "# Train Word2Vec embeddings\n",
    "model = Word2Vec(sentences=tokenized_text, vector_size=50, window=7, min_count=7, workers=10)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('models/first_word2vec.model')\n",
    "\n",
    "# Load pre-trained word embeddings (Word2Vec or GloVe)\n",
    "word_embeddings = Word2Vec.load('models/first_word2vec.model').wv\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = word_embeddings.vector_size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_embeddings.key_to_index:\n",
    "        embedding_matrix[i] = word_embeddings[word]\n",
    "\n",
    "# Prepare the input and output data\n",
    "X = sequences\n",
    "y = df['overall_risk'].values\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predictions for the next 7 days:\n",
      "Day 1: 2.3292155265808105\n",
      "Day 2: 2.3292155265808105\n",
      "Day 3: 2.3292155265808105\n",
      "Day 4: 2.3292155265808105\n",
      "Day 5: 2.3292155265808105\n",
      "Day 6: 2.3292155265808105\n",
      "Day 7: 2.3292155265808105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict for the next 7 days\n",
    "last_sequence = sequences[-1]  # Get the last sequence from the input data\n",
    "\n",
    "predictions = []\n",
    "for _ in range(7):\n",
    "    # next_sequence = np.concatenate([last_sequence[1:], np.zeros(1)])  # Shift the sequence by one position\n",
    "    next_sequence = next_sequence.reshape(1, -1)  # Reshape to match the input shape\n",
    "    prediction = model.predict(next_sequence)[0][0]  # Make the prediction\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # Update the last sequence with the predicted value\n",
    "    last_sequence[-1] = prediction\n",
    "    last_sequence = last_sequence.reshape(1, -1)\n",
    "\n",
    "# Print the predictions for the next 7 days\n",
    "print(\"Predictions for the next 7 days:\")\n",
    "for i, prediction in enumerate(predictions, 1):\n",
    "    print(f\"Day {i}: {prediction}\")\n",
    "\n",
    "# Chart the predictions\n",
    "days = range(1, 8)\n",
    "risk_levels = predictions\n",
    "\n",
    "p = figure(title=\"Risk Level Predictions for Next 7 Days\", x_axis_label=\"Day\", y_axis_label=\"Risk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (820452495.py, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[46], line 83\u001b[0;36m\u001b[0m\n\u001b[0;31m    y_train_pred[start_idx\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "# Load the dataframe\n",
    "# df = pd.read_csv('your_dataframe.csv')\n",
    "df = data\n",
    "\n",
    "# Select the features you want to use for training\n",
    "features = ['tavg', 'tmin', 'tmax', 'prcp', 'wdir', 'pres', 'tsun']\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])  # Normalize the numerical features\n",
    "\n",
    "# Convert text tokens to numerical sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tokens'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['tokens'])\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "\n",
    "tokenized_text = [str(tokens).split() for tokens in df['tokens']]\n",
    "# Train Word2Vec embeddings\n",
    "model = Word2Vec(sentences=tokenized_text, vector_size=50, window=7, min_count=7, workers=10)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('models/first_word2vec.model')\n",
    "\n",
    "# Load pre-trained word embeddings (Word2Vec or GloVe)\n",
    "word_embeddings = Word2Vec.load('models/first_word2vec.model').wv\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = word_embeddings.vector_size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_embeddings.key_to_index:\n",
    "        embedding_matrix[i] = word_embeddings[word]\n",
    "\n",
    "# Prepare the input and output data\n",
    "X = sequences\n",
    "y = df['overall_risk'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=8)\n",
    "\n",
    "# Predict on the second half of the training data\n",
    "X_train_pred = model.predict(X_train[train_size:])\n",
    "y_train_pred = X_train_pred.flatten()\n",
    "\n",
    "# Plot the predictions and actual results\n",
    "output_file(\"predictions_plot.html\")\n",
    "p = figure(title=\"Model Predictions vs Actual Results\", x_axis_label=\"Data Index\", y_axis_label=\"Overall Risk\")\n",
    "p.line(range(train_size, len(y_train)), y_train_pred, legend_label=\"Predictions\", line_color=\"red\")\n",
    "p.line(range(train_size, len(y_train)), y_train, legend_label=\"Actual\", line_color=\"blue\")\n",
    "show(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('x', 0), ('y', 116)\n",
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('x', 0), ('y', 116)\n",
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/jaymin/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/jaymin/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the training loss\n",
    "output_file(\"loss_plot.html\")\n",
    "p1 = figure(title=\"Training Loss\", x_axis_label=\"Epoch\", y_axis_label=\"Loss\")\n",
    "p1.line(range(1, len(history.history['loss']) + 1), history.history['loss'])\n",
    "# Saving the plot to the output_file:\n",
    "show(p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
