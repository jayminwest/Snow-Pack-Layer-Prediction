{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing of Avalanche Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Password: "
     ]
    }
   ],
   "source": [
    "import os, keys\n",
    "\n",
    "os.system(\"\"\"\n",
    "psql -h %s -p %s -U %s -d %s -W\n",
    "\"\"\"%(keys.PG_HOST, keys.PG_PORT, keys.PG_USER, keys.PG_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>zone</th>\n",
       "      <th>overall_risk</th>\n",
       "      <th>above_treeline_risk</th>\n",
       "      <th>near_treeline_risk</th>\n",
       "      <th>below_treeline_risk</th>\n",
       "      <th>bottom_line_text</th>\n",
       "      <th>problem_type_text</th>\n",
       "      <th>forecast_discussion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>Mt Hood</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>As new snow starts piling up, think about avoi...</td>\n",
       "      <td>An approaching storm will bring moderate preci...</td>\n",
       "      <td>This incoming system is expected to hit the Mt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>East Slopes South</td>\n",
       "      <td>LOW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Generally safe avalanche conditions exist.  Ho...</td>\n",
       "      <td>A skiff of new snow and a good dose of strong ...</td>\n",
       "      <td>The main story for Sunday will likely be the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>East Slopes North</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A weak storm will bring light rain below treel...</td>\n",
       "      <td>Sunday's storm will bring more wind than snow,...</td>\n",
       "      <td>Access in the East North zone is difficult and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>West Slopes South</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>You may see the wind building new slabs throug...</td>\n",
       "      <td>You may not find any wind slabs to start the d...</td>\n",
       "      <td>Right off the bat in the morning, you may not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-15</td>\n",
       "      <td>Snoqualmie Pass</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A couple of inches of new snow in the afternoo...</td>\n",
       "      <td>You may see a wind slab problem start to devel...</td>\n",
       "      <td>First thing Sunday morning, you may find gener...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date               zone overall_risk  above_treeline_risk  \\\n",
       "0  2023-04-15            Mt Hood     MODERATE                  2.0   \n",
       "1  2023-04-15  East Slopes South          LOW                  1.0   \n",
       "2  2023-04-15  East Slopes North     MODERATE                  2.0   \n",
       "3  2023-04-15  West Slopes South     MODERATE                  2.0   \n",
       "4  2023-04-15    Snoqualmie Pass     MODERATE                  2.0   \n",
       "\n",
       "   near_treeline_risk  below_treeline_risk  \\\n",
       "0                 2.0                  1.0   \n",
       "1                 1.0                  1.0   \n",
       "2                 1.0                  1.0   \n",
       "3                 2.0                  1.0   \n",
       "4                 1.0                  1.0   \n",
       "\n",
       "                                    bottom_line_text  \\\n",
       "0  As new snow starts piling up, think about avoi...   \n",
       "1  Generally safe avalanche conditions exist.  Ho...   \n",
       "2  A weak storm will bring light rain below treel...   \n",
       "3  You may see the wind building new slabs throug...   \n",
       "4  A couple of inches of new snow in the afternoo...   \n",
       "\n",
       "                                   problem_type_text  \\\n",
       "0  An approaching storm will bring moderate preci...   \n",
       "1  A skiff of new snow and a good dose of strong ...   \n",
       "2  Sunday's storm will bring more wind than snow,...   \n",
       "3  You may not find any wind slabs to start the d...   \n",
       "4  You may see a wind slab problem start to devel...   \n",
       "\n",
       "                            forecast_discussion_text  \n",
       "0  This incoming system is expected to hit the Mt...  \n",
       "1  The main story for Sunday will likely be the g...  \n",
       "2  Access in the East North zone is difficult and...  \n",
       "3  Right off the bat in the morning, you may not ...  \n",
       "4  First thing Sunday morning, you may find gener...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('output_data/incomplete_All_Zones_Current_Season_reports_data.csv')\n",
    "all_data.columns = columns=['date', 'zone', 'overall_risk', 'above_treeline_risk', 'near_treeline_risk', 'below_treeline_risk', 'bottom_line_text', 'problem_type_text', 'forecast_discussion_text']\n",
    "all_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and using an LDA Model on the data:\n",
    "- Should compare the differences between teh tree columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the LDA Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaymin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jaymin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jaymin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from gensim import corpora, models\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # adding days of the week to stop words\n",
    "    stop_words.update(['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "    # adding months to stop words\n",
    "    stop_words.update(['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "                       'september', 'october', 'november', 'december'])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = simple_preprocess(str(words), deacc=True)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def prepare_text_column(column):\n",
    "    \"\"\"\n",
    "    Prepares a text column for LDA analysis.\n",
    "    \"\"\"\n",
    "    column = [str(item) for item in column]\n",
    "    processed = [preprocess(doc) for doc in column]\n",
    "\n",
    "    # Create a dictionary of terms and their frequency\n",
    "    dictionary = corpora.Dictionary(processed)\n",
    "\n",
    "    # Create a document-term matrix\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed]\n",
    "\n",
    "    return doc_term_matrix, dictionary\n",
    "\n",
    "# Load the dataset\n",
    "bl_matrix, bl_dict = prepare_text_column(all_data['bottom_line_text'].to_list())\n",
    "pt_matrix, pt_dict = prepare_text_column(all_data['problem_type_text'].to_list())\n",
    "fd_matrix, fd_dict = prepare_text_column(all_data['forecast_discussion_text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jaymin/Desktop/Spring 2023/CPSC325/SnowPackPrediction/nlp_processing.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# show the plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m gensim_dict_to_wordmap(bl_dict)\n",
      "\u001b[1;32m/home/jaymin/Desktop/Spring 2023/CPSC325/SnowPackPrediction/nlp_processing.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m font_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfonts/Arial.ttf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# generate the word cloud from the dictionary\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m wordcloud \u001b[39m=\u001b[39m WordCloud(width\u001b[39m=\u001b[39;49m\u001b[39m800\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m, max_words\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, background_color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m, font_path\u001b[39m=\u001b[39;49mfont_path)\u001b[39m.\u001b[39;49mgenerate_from_frequencies(word_freq)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# plot the WordCloud image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jaymin/Desktop/Spring%202023/CPSC325/SnowPackPrediction/nlp_processing.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), facecolor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m     font_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(\u001b[39mdict\u001b[39;49m(frequencies[:\u001b[39m2\u001b[39;49m]),\n\u001b[1;32m    454\u001b[0m                                    max_font_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight)\n\u001b[1;32m    455\u001b[0m     \u001b[39m# find font sizes\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     sizes \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout_]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    505\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[1;32m    506\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[1;32m    507\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    509\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[1;32m    510\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[1;32m    511\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[1;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    669\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n\u001b[1;32m    670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(font, ImageFont\u001b[39m.\u001b[39mFreeTypeFont):\n\u001b[0;32m--> 671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly supported for TrueType fonts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m embedded_color \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmode\n\u001b[1;32m    673\u001b[0m bbox \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mgetbbox(\n\u001b[1;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[1;32m    675\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def gensim_dict_to_wordmap(dictionary):\n",
    "    word_freq = {dictionary[id]: freq for id, freq in dictionary.cfs.items()}\n",
    "    \n",
    "    font_path = 'fonts/Arial.ttf'\n",
    "    # generate the word cloud from the dictionary\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=50, background_color='white', font_path=font_path).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    \n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "gensim_dict_to_wordmap(bl_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the LDA Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LDA models\n",
    "# bl_model = LdaModel(bl_matrix, num_topics=5, id2word=bl_dict, passes=10)\n",
    "pd_model = LdaModel(pt_matrix, num_topics=5, id2word=pt_dict, passes=10)\n",
    "# fd_model = LdaModel(fd_matrix, num_topics=10, id2word=fd_dict, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.061*\"wind\" + 0.048*\"slab\" + 0.031*\"snow\" + 0.014*\"slope\" + 0.012*\"new\" + 0.011*\"avalanche\" + 0.011*\"could\" + 0.010*\"terrain\" + 0.009*\"loaded\" + 0.008*\"find\"\n",
      "Topic: 1 \n",
      "Words: 0.049*\"wind\" + 0.044*\"snow\" + 0.028*\"slab\" + 0.028*\"slope\" + 0.018*\"avalanche\" + 0.018*\"terrain\" + 0.013*\"steep\" + 0.012*\"surface\" + 0.011*\"trigger\" + 0.011*\"could\"\n",
      "Topic: 2 \n",
      "Words: 0.039*\"snow\" + 0.039*\"avalanche\" + 0.036*\"wet\" + 0.023*\"slope\" + 0.017*\"could\" + 0.016*\"loose\" + 0.013*\"slab\" + 0.012*\"slide\" + 0.011*\"surface\" + 0.011*\"steep\"\n",
      "Topic: 3 \n",
      "Words: 0.023*\"avalanche\" + 0.021*\"day\" + 0.020*\"slope\" + 0.020*\"storm\" + 0.014*\"snow\" + 0.013*\"large\" + 0.013*\"slab\" + 0.013*\"could\" + 0.012*\"wind\" + 0.010*\"steep\"\n",
      "Topic: 4 \n",
      "Words: 0.039*\"snow\" + 0.034*\"avalanche\" + 0.022*\"wind\" + 0.021*\"slab\" + 0.021*\"could\" + 0.016*\"slope\" + 0.014*\"steep\" + 0.011*\"terrain\" + 0.010*\"wet\" + 0.010*\"new\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the topics and their corresponding keywords\n",
    "for idx, topic in pd_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "input_ids = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "features = outputs.last_hidden_state\n",
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
